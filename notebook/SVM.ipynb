{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb196b7e-5f19-4bda-8ffe-67db5898afa4",
   "metadata": {},
   "source": [
    "## A Support Vector Machine (SVM) \n",
    "SVM is a supervised machine learning algorithm widely used for both classification and regression tasks. SVMs work by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space. This hyperplane maximizes the margin between classes, aiming to create a clear boundary between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698229e0-15d1-400a-9972-242c0438f588",
   "metadata": {},
   "source": [
    "## Hard Margin vs. Soft Margin:\n",
    "Hard Margin SVM: Used when data is linearly separable, meaning the classes can be perfectly divided by a hyperplane. Here, the algorithm aims for a margin that separates all data points correctly, with no tolerance for misclassification. However, this approach is sensitive to outliers, as any misclassified point would prevent the model from finding a separating hyperplane.\n",
    "\n",
    "Soft Margin SVM: Used when data is not perfectly separable, allowing some points to fall on the wrong side of the hyperplane to achieve a better overall solution. This approach introduces a penalty term for misclassified points, which balances maximizing the margin and minimizing classification errors, making it more robust to noise and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cf002-e2d5-4d9b-9c5a-0e2ba0c6a739",
   "metadata": {},
   "source": [
    "Overall, SVMs are powerful for both linear and non-linear tasks, with the flexibility to handle complex datasets through the use of kernel functions that map data into higher dimensions where it becomes more easily separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e88f4-f161-43b3-bbda-1e7745da73b0",
   "metadata": {},
   "source": [
    "### 1. Mathematical Formulation for Classification\n",
    "In SVM classification, given a dataset of $n$ points $(x_i, y_i)$ where $x_i$ is the feature vector and $y_i$ is the label ($y_i = +1$ or $y_i = -1$), we aim to find a hyperplane defined by:\n",
    "$$\n",
    "f(x) = w \\cdot x + b = 0\n",
    "$$\n",
    "where $w$ is the weight vector, and $b$ is the bias. The goal is to maximize the margin, or distance between the hyperplane and the closest data points from each class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd9ea8-3c5f-49ae-a1e2-2c5f27441a69",
   "metadata": {},
   "source": [
    "### 2. Objective: Maximizing the Margin\n",
    "The margin is $\\frac{2}{\\|w\\|}$, so to maximize it, we minimize $\\|w\\|^2$, subject to the constraint that all points are correctly classified:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "This is the **hard margin** SVM formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6654604-cbcd-4a21-b717-80bce9315c38",
   "metadata": {},
   "source": [
    "### 3. Soft Margin SVM\n",
    "In cases where data is not perfectly separable, we introduce slack variables $\\xi_i$ to allow for some misclassification. The objective then becomes:\n",
    "$$\n",
    "\\min \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "subject to $y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i$ for all $i$, where $C$ is a regularization parameter that controls the trade-off between maximizing the margin and minimizing misclassification. Larger values of $C$ put more emphasis on minimizing misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8335e5-c3d2-4d78-9732-614f7e72c57f",
   "metadata": {},
   "source": [
    "### 4. Loss Function for Classification (Hinge Loss)\n",
    "The loss function commonly used in SVM classification is the **hinge loss**, given by:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^n \\max(0, 1 - y_i (w \\cdot x_i + b))\n",
    "$$\n",
    "The hinge loss penalizes points that are within the margin or on the wrong side of the hyperplane. This formulation allows SVMs to focus on correctly classifying points that are closer to the decision boundary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6149c-8025-4d76-b0a0-544fca50da17",
   "metadata": {},
   "source": [
    "### 5. SVM for Regression (Support Vector Regression - SVR)\n",
    "In regression, the objective is to find a function $f(x) = w \\cdot x + b$ that approximates the target variable as closely as possible within a certain margin of tolerance $\\epsilon$. The goal is to ensure that most data points lie within an $\\epsilon$-distance of the predicted function, forming a \"tube\" around the regression line.\n",
    "\n",
    "### 6. Loss Function for Regression (ε-Insensitive Loss)\n",
    "The **ε-insensitive loss** function used in SVR ignores errors within the $\\epsilon$-margin but penalizes those outside it. This is defined as:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^n \\max(0, |y_i - (w \\cdot x_i + b)| - \\epsilon)\n",
    "$$\n",
    "This loss function allows the SVM to approximate the target variable while ignoring small errors within the $\\epsilon$-tube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dbb6c-c7a4-4f9f-8c30-d219d174cb85",
   "metadata": {},
   "source": [
    "### Summary of Loss Functions:\n",
    "- **Classification (Hinge Loss)**: $\\sum \\max(0, 1 - y_i (w \\cdot x_i + b))$\n",
    "- **Regression (ε-Insensitive Loss)**: $\\sum \\max(0, |y_i - (w \\cdot x_i + b)| - \\epsilon)$\n",
    "\n",
    "In both cases, the SVM aims to find an optimal balance between minimizing classification or regression error and maximizing the margin (for classification) or maintaining an error tolerance (for regression)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
